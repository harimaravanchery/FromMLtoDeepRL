{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"14d2G_-iPU-JD23tfb5J9LX7GmTUmgn0F","timestamp":1721401979155}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jqGbK5ZBg6Uf"},"source":["# Machine Learning and AI for Autonomous Systems\n","## A program by IISc and TalentSprint\n","### Assignment: Double DQN in CartPole enviroment"]},{"cell_type":"markdown","metadata":{"id":"fK_cVGGx4JoO"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"k6s6kEQH4Lye"},"source":["At the end of the experiment, you will be able to:\n","\n","* understand the Double DQN Algorithm.\n","* implementation of DDQN algorithm in CartPole environment."]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"]},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"2302794\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjoZJWGErxGf"},"source":["#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"9008710123\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":34},"id":"kxgfakokDUbI","outputId":"3018803b-1c06-48b4-9ef6-828ac148fc07"},"outputs":[{"data":{"text/html":["<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=aias_02&recordId=962\"></script>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Setup completed successfully\n"]}],"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","\n","notebook= \"AIAS_B2_M4_AST_07_DDQN_CartPole_C\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://aias-iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError\n","    else:\n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xqcsYSOi4rIF"},"source":["## Introduction"]},{"cell_type":"markdown","source":["### The Cartpole Environment: Balancing a Pole on a Cart\n","\n","The Cartpole environment is a classic benchmark task in the field of reinforcement learning (RL). It's widely used because it's:\n","\n","* **Simple:** Easy to understand and implement, making it a great starting point for learning RL concepts.\n","* **Challenging:** Mastering the task requires learning effective control strategies, making it a good testbed for various algorithms.\n","* **Informative:** Offers valuable insights into the strengths and weaknesses of different RL approaches.\n","\n","Here's a deeper dive into the Cartpole environment:\n","\n","**The Setup:**\n","\n","* Imagine a pole attached by a hinge to a cart moving freely on a frictionless track.\n","* The agent can apply force to the cart in either direction (left or right).\n","* The goal is to balance the pole upright for as long as possible by applying appropriate forces based on observations.\n","\n","**Observations:**\n","\n","The agent receives four observations at each step:\n","\n","1. **Cart position:** Where the cart is located on the track (positive for right, negative for left).\n","2. **Cart velocity:** How fast the cart is moving.\n","3. **Pole angle:** The angle of the pole with respect to the vertical (positive for tilting to the right, negative for tilting to the left).\n","4. **Pole velocity:** How fast the angle of the pole is changing.\n","\n","The continuous state space is an X coordinate for location, the velocity of the cart, the angle of the pole, and the velocity at the tip of the pole. The X coordinate goes from -4.8 to +4.8, velocity is -Inf to +Inf, angle of the pole goes from -24 degrees to +24 degrees, tip velocity is -Inf to +Inf. With all of the possible combinations you can see why we can't create a Q table for each one.\n","\n","**Actions:**\n","\n","The agent can choose one of two actions at each step:\n","\n","1. **Push cart to the right:** Applies a force to the right side of the cart.\n","2. **Push cart to the left:** Applies a force to the left side of the cart.\n","\n","The actions are 0 to push the cart to the left and 1 to push the cart to the right.\n","\n","**Rewards:**\n","\n","The agent receives a reward of +1 for every timestep the pole stays upright. No additional reward is given for pushing the cart in either direction.\n","\n","\n","\n","**Challenges:**\n","\n","* The Cartpole environment is unstable and highly nonlinear. Small changes in the cart's position or velocity can significantly affect the pole's balance.\n","* The agent needs to learn a fine-grained control strategy to apply just the right amount of force at the right time to keep the pole balanced.\n","* There are multiple solutions to the task, so the agent needs to discover an effective policy that works consistently under different initial conditions.\n","\n","\n","\n","\n","To \"solve\" this puzzle you have to have an average reward of > 195 over 100 consecutive episodes.\n","\n","**Applications:**\n","\n","The Cartpole environment serves as a stepping stone for various RL applications, including:\n","\n","* **Control systems:** Learning to stabilize real-world systems like robots or drones.\n","* **Dynamic decision-making:** Making optimal choices in situations with changing dynamics and uncertain outcomes.\n","* **Policy optimization:** Developing efficient algorithms for learning control strategies from rewards and observations.\n","\n","By mastering the Cartpole environment, RL learners gain valuable experience and insights applicable to more complex tasks in diverse domains.\n","\n","\n","\n"],"metadata":{"id":"dXlCsqZ5Z0qF"}},{"cell_type":"markdown","metadata":{"id":"Psn-z1duUNmA"},"source":["### Import required packages"]},{"cell_type":"code","source":["#Imports and gym creation\n","import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import deque\n","import tensorflow as tf\n","from tensorflow import keras\n","#import stable_baselines3\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.optimizers import Adam\n","import random\n","\n"],"metadata":{"id":"B02hCYsRbrSI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Create Gym\n","from gym import wrappers\n","envCartPole = gym.make('CartPole-v1')\n","envCartPole.seed(50)"],"metadata":{"id":"O1pGaRyq2aKU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"069a59d0-9111-427e-82fb-895f51503932"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n"]},{"output_type":"execute_result","data":{"text/plain":["[50]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58MQuUfMG1X7"},"outputs":[],"source":["EPISODES = 500\n","TRAIN_END = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpiXtiNQG1X8"},"outputs":[],"source":["def discount_rate(): #Gamma\n","    return 0.95\n","\n","def learning_rate(): #Alpha\n","    return 0.001\n","\n","def batch_size():\n","    return 24"]},{"cell_type":"markdown","source":["### DOUBLE DEEP Q-LEARNING\n","\n","\n","\n","Double Deep Q-Learning (DDQN) is an improvement over Deep Q-Learning (DQN) that **reduces overestimation bias** in action-value estimates. This bias can lead to suboptimal decisions by the agent, as it often overestimates the rewards expected from taking certain actions. Before discussing about DDQN lets discuss some brief about Q-Learning, and DQN.\n","\n","\n","Deep Q-Learning (DQN) is a powerful reinforcement learning algorithm that combines Q-learning with deep neural networks. It allows agents to learn optimal actions in complex environments with large state and action spaces. Here's a breakdown of the key concepts:\n","\n","**1. Q-Learning:**\n","\n","Q-learning is a **model-free** reinforcement learning algorithm that learns **action-value functions**, also known as **Q-values**. These Q-values estimate the **expected future reward** of taking a specific action in a given state. Over time, the agent updates its Q-values based on its experiences, ultimately aiming to choose actions that maximize its long-term rewards.\n","\n","**2. Deep Neural Networks:**\n","\n","DQN replaces the traditional Q-value lookup table with a **deep neural network**. This network takes the current state of the environment as input and outputs a Q-value for each possible action. By training this network on experience data, DQN can learn complex Q-functions that adapt to diverse environments.\n","\n","**3. Training Process:**\n","\n","Here's how DQN learns:\n","\n","* **Experience replay:** The agent stores its experiences in a replay memory, consisting of transitions between states, actions, rewards, and next states. This allows the agent to learn from past experiences more efficiently than just relying on the current state.\n","* **Batch learning:** The agent samples mini-batches of experiences from the replay memory and uses them to update its Q-network. This update minimizes the **Bellman equation**, which relates the Q-value of a state-action pair to the expected future rewards obtained by taking that action.\n","* **Target network:** To reduce overestimation bias, DQN employs a **target network** that is a slow copy of the main Q-network. The target network is used to evaluate the Q-values predicted by the main network during updates, leading to more stable and accurate learning.\n","\n","**4. Strengths:**\n","\n","* **Scalability:** DQN can handle large state and action spaces effectively, making it suitable for complex tasks.\n","* **Generalization:** The deep neural network allows DQN to generalize from past experiences to unseen situations, improving its adaptability.\n","* **Sample efficiency:** By using experience replay, DQN can learn effectively even with limited interaction with the environment.\n","\n","**5. Weaknesses:**\n","\n","* **Computational cost:** Training deep neural networks can be computationally expensive and require significant resources.\n","* **Hyperparameter tuning:** Selecting the optimal hyperparameters for the deep network and training process can be challenging.\n","* **Overestimation bias:** DQN can overestimate Q-values due to the correlation between action selection and evaluation within the same network. Double Deep Q-Learning (DDQN) addresses this issue by using separate networks for selection and evaluation.\n","\n","**Overall, DQN is a powerful reinforcement learning algorithm that has revolutionized the field. Its ability to learn complex Q-functions from experience makes it a valuable tool for tackling diverse tasks in areas like robotics, video game playing, and resource management.**\n","\n","\n","\n","Here's the core idea behind DDQN:\n","\n","**1. Two Q-networks:** DDQN utilizes **two separate Q-networks**:\n","\n","- **Main Q-network:** This network is used to **select the best action** in a given state based on its estimated Q-values.\n","- **Target Q-network:** This network is a slower copy of the main network, used to **evaluate the Q-values** of the actions chosen by the main network. Updating the target network happens periodically by copying the weights of the main network.\n","\n","\n","DDQN overcomes this by **decoupling action selection and evaluation**:\n","\n","- The **main network chooses the action with the highest Q-value** according to its own estimates.\n","- The **target network evaluates the Q-value of this chosen action**.\n","\n","This separation reduces the bias, as the network used for evaluation is not influenced by the selection process.\n","\n","**3. Benefits of DDQN:**\n","\n","- Improved convergence and stability during training.\n","- Better performance in complex tasks with long reward horizons.\n","- More robust to overestimation bias compared to DQN.\n","\n","**4. Implementation:**\n","\n","DDQN uses the same update process as DQN, with the exception of using the separate target network for evaluation:\n","\n","**example**\n","\n","\n","Imagine you're playing a maze game where you need to collect coins and reach the exit. You have two maps:\n","\n","- **Map 1 (Main Map):** You use this map to find the currently best path based on your estimated coin values.\n","- **Map 2 (Double Map):** This map is a slower update of Map 1 and you use it to verify the actual value of the path you picked from Map 1.\n","\n","**Scenario:**\n","\n","1. You're at a fork in the road on Map 1. Both paths seem to lead to coins, but one path has a slightly higher estimated value on Map 1.\n","2. You take the path with the higher estimated value, following your \"greedy\" strategy based on Map 1.\n","3. However, when you reach that path on Map 2, you discover there are fewer coins than Map 1 estimated. This is the overestimation bias!\n","\n","\n","**How DDQN fixes this:**\n","\n","- In DDQN, you use the **Double Map (Target Network)** to evaluate the path chosen by the **Main Map (Main Network)**, not the same map that chose it. This separation prevents the overestimation bias.\n","- By verifying the value of the chosen path through the Double Map, you get a more accurate picture of its true reward. This helps you learn better and make more optimal decisions in the future.\n","\n","**Benefits in the Maze Game:**\n","\n","- Using DDQN, you're less likely to be fooled by misleading paths with inflated values on the Main Map.\n","- You learn to trust the verified paths from the Double Map, leading to faster and more efficient coin collection.\n","- Overall, you perform better and reach the exit quicker compared to relying solely on the Main Map's estimates.\n","\n"," **Here's another example to illustrate DDQN, this time in a financial investment setting:**\n","\n","**Imagine you're an investment agent tasked with allocating funds across different assets to maximize returns. You have two models:**\n","\n","- **Main Model:** This model predicts the expected returns of each asset based on current market conditions.\n","- **Target Model:** This model is a slower-updating copy of the Main Model, used to evaluate the predicted returns more cautiously.\n","\n","**Scenario:**\n","\n","1. The Main Model predicts that Asset A has a potential return of 10%, while Asset B has a predicted return of 8%.\n","2. Following a greedy strategy, you allocate more funds to Asset A, expecting higher returns.\n","3. However, the Target Model, with its more conservative estimates, predicts that Asset A's actual return is likely closer to 7%, while Asset B's return is closer to 8%.\n","\n","**How DDQN addresses this:**\n","\n","- DDQN uses the Target Model's evaluation to adjust the Main Model's predictions, reducing the overestimation bias.\n","- In this case, the Main Model learns to temper its expectations for Asset A, leading to a more balanced allocation strategy.\n","\n","**Benefits in Investment:**\n","\n","- DDQN helps prevent over-allocation to assets with potentially inflated returns, reducing risk exposure.\n","- It encourages more realistic return expectations, leading to more stable and sustainable portfolio growth.\n","- It improves overall investment performance by mitigating overestimation bias.\n","\n","\n","\n","\n","\n","\n","**Remember:**\n","\n","- DDQN is like having a trusted friend double-check your decisions before you commit to them.\n","- This double-checking reduces bias and leads to better learning and performance.\n","\n","\n","\n","\n","**Summary:**\n","\n","Double Deep Q-Learning is a powerful technique for reinforcement learning that reduces overestimation bias and improves performance in complex tasks. Its use of separate Q-networks for action selection and evaluation makes it a valuable tool for agents to learn optimal behaviors more effectively.\n","\n","\n","\n"],"metadata":{"id":"cOg3ZYW1cfWx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1v6Y6C1OG1X_"},"outputs":[],"source":["class DoubleDeepQNetwork():\n","    def __init__(self, states, actions, alpha, gamma, epsilon,epsilon_min, epsilon_decay):\n","        self.nS = states\n","        self.nA = actions\n","        self.memory = deque([], maxlen=2500)\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        #Explore/Exploit\n","        self.epsilon = epsilon\n","        self.epsilon_min = epsilon_min\n","        self.epsilon_decay = epsilon_decay\n","        self.model = self.build_model()\n","        self.model_target = self.build_model() #Second (target) neural network\n","        self.update_target_from_model() #Update weights\n","        self.loss = []\n","\n","    def build_model(self):\n","        model = keras.Sequential() #linear stack of layers https://keras.io/models/sequential/\n","        model.add(keras.layers.Dense(24, input_dim=self.nS, activation='relu')) #[Input] -> Layer 1\n","        #   Dense: Densely connected layer https://keras.io/layers/core/\n","        #   24: Number of neurons\n","        #   input_dim: Number of input variables\n","        #   activation: Rectified Linear Unit (relu) ranges >= 0\n","        model.add(keras.layers.Dense(24, activation='relu')) #Layer 2 -> 3\n","        model.add(keras.layers.Dense(self.nA, activation='linear')) #Layer 3 -> [output]\n","        #   Size has to match the output (different actions)\n","        #   Linear activation on the last layer\n","        model.compile(loss='mean_squared_error', #Loss function: Mean Squared Error\n","                      optimizer=keras.optimizers.Adam(lr=self.alpha)) #Optimaizer: Adam (Feel free to check other options)\n","        return model\n","\n","    def update_target_from_model(self):\n","        #Update the target model from the base model\n","        self.model_target.set_weights( self.model.get_weights() )\n","\n","    def action(self, state):\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.nA) #Explore\n","        action_vals = self.model.predict(state) #Exploit: Use the NN to predict the correct action from this state\n","        return np.argmax(action_vals[0])\n","\n","    def test_action(self, state): #Exploit\n","        action_vals = self.model.predict(state)\n","        return np.argmax(action_vals[0])\n","\n","    def store(self, state, action, reward, nstate, done):\n","        #Store the experience in memory\n","        self.memory.append( (state, action, reward, nstate, done) )\n","\n","    def experience_replay(self, batch_size):\n","        #Execute the experience replay\n","\n","        #1. Sample a Mini - Batch.\n","        minibatch = random.sample( self.memory, batch_size ) #Randomly sample from memory\n","\n","        #2. Prepare State and Next State arrays\n","        #Convert to numpy for speed by vectorization\n","        st = np.zeros((batch_size, self.nS))  # States\n","        nst = np.zeros((batch_size, self.nS))  # Next States\n","        for i in range(batch_size):  # Creating the state and next state np arrays\n","          st[i] = minibatch[i][0].reshape(-1, self.nS)\n","          nst[i] = minibatch[i][3].reshape(-1, self.nS)\n","\n","        #3. Predict Q-Values\n","        st_predict = self.model.predict(st,verbose=0) #Here is the speedup! I can predict on the ENTIRE batch\n","        nst_predict = self.model.predict(nst,verbose=0)\n","        nst_predict_target = self.model_target.predict(nst,verbose=0) #Predict from the TARGET\n","\n","        #4. Compute Target Q-Values\n","        x = []\n","        y = []\n","        index = 0\n","        for state, action, reward, nstate, done in minibatch:\n","            x.append(state)\n","            #Predict from state\n","            nst_action_predict_target = nst_predict_target[index]\n","            nst_action_predict_model = nst_predict[index]\n","            if done == True: #Terminal: Just assign reward much like {* (not done) - QB[state][action]}\n","                target = reward\n","            else:   #Non terminal\n","                target = reward + self.gamma * nst_action_predict_target[np.argmax(nst_action_predict_model)] #Using Q to get T is Double DQN\n","            target_f = st_predict[index]\n","            target_f[action] = target\n","            y.append(target_f)\n","            index += 1\n","        #Reshape for Keras Fit\n","        x_reshape = np.array(x).reshape(batch_size,self.nS)\n","        y_reshape = np.array(y)\n","        epoch_count = 1\n","        hist = self.model.fit(x_reshape, y_reshape, epochs=epoch_count, verbose=0)\n","        #Graph Losses\n","        for i in range(epoch_count):\n","            self.loss.append( hist.history['loss'][i] )\n","        #Decay Epsilon\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay"]},{"cell_type":"markdown","source":["#### Create the agent for training"],"metadata":{"id":"_Ko33fg_hP-I"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwZcpuRlG1YC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f0a5b24e-1508-4084-b202-ca11a58852bb"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n","WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]}],"source":["#Create the agents\n","nS = envCartPole.observation_space.shape[0] #This is only 4\n","nA = envCartPole.action_space.n #Actions\n","dqn = DoubleDeepQNetwork(nS, nA, learning_rate(), discount_rate(), 1, 0.001, 0.995 )\n","\n","batch_size = batch_size()"]},{"cell_type":"markdown","source":["#### Training the model"],"metadata":{"id":"iTfSeftphtMh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jjDj6sgG1YD","outputId":"d45d4863-3174-4ec2-bf3c-fa9a19b1ee0a","colab":{"base_uri":"https://localhost:8080/","height":473}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n","If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"output_type":"stream","name":"stdout","text":["episode: 0/500, score: 12.0, e: 1\n"]},{"output_type":"error","ename":"ValueError","evalue":"setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (24, 5) + inhomogeneous part.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-99ae5ae5a67d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#Experience Replay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m#Update the weights after each episode (You can configure this for x steps as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-b111be12c658>\u001b[0m in \u001b[0;36mexperience_replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mnp_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#States\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mnst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;31m#Next States\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (24, 5) + inhomogeneous part."]}],"source":["#Training\n","rewards = [] #Store rewards for graphing\n","epsilons = [] # Store the Explore/Exploit\n","TEST_Episodes = 0\n","for e in range(EPISODES):\n","    state = envCartPole.reset()\n","    state = np.reshape(state, [1, nS]) # Resize to store in memory to pass to .predict\n","    tot_rewards = 0\n","    for time in range(210): #200 is when you \"solve\" the game. This can continue forever as far as I know\n","        envCartPole.render()\n","        action = dqn.action(state)\n","        nstate, reward, done, _ = envCartPole.step(action)\n","        nstate = np.reshape(nstate, [1, nS])\n","        tot_rewards += reward\n","        dqn.store(state, action, reward, nstate, done) # Resize to store in memory to pass to .predict\n","        state = nstate\n","        #done: CartPole fell.\n","        #time == 209: CartPole stayed upright\n","        if done or time == 209:\n","            rewards.append(tot_rewards)\n","            epsilons.append(dqn.epsilon)\n","            print(\"episode: {}/{}, score: {}, e: {}\"\n","                  .format(e, EPISODES, tot_rewards, dqn.epsilon))\n","            break\n","        #Experience Replay\n","        if len(dqn.memory) > batch_size:\n","            dqn.experience_replay(batch_size)\n","    #Update the weights after each episode (You can configure this for x steps as well\n","    dqn.update_target_from_model()\n","    #If our current NN passes we are done\n","    #I am going to use the last 5 runs\n","    if len(rewards) > 5 and np.average(rewards[-5:]) > 195:\n","        #Set the rest of the EPISODES for testing\n","        TEST_Episodes = 100    # EPISODES - e\n","        TRAIN_END = e\n","        break"]},{"cell_type":"markdown","source":["### Testing"],"metadata":{"id":"jS-tsTjS4mEV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4R0QcrlG1YE"},"outputs":[],"source":["#Testing\n","\n","#TEST Time\n","#   In this section we ALWAYS use exploit don't train any more\n","for e_test in range(TEST_Episodes):\n","    state = envCartPole.reset()\n","    state = np.reshape(state, [1, nS])\n","    tot_rewards = 0\n","    for t_test in range(210):\n","        action = dqn.test_action(state)\n","        nstate, reward, done, _ = envCartPole.step(action)\n","        nstate = np.reshape( nstate, [1, nS])\n","        tot_rewards += reward\n","        #DON'T STORE ANYTHING DURING TESTING\n","        state = nstate\n","        #done: CartPole fell.\n","        #t_test == 209: CartPole stayed upright\n","        if done or t_test == 209:\n","            rewards.append(tot_rewards)\n","            epsilons.append(0) #We are doing full exploit\n","            print(\"episode: {}/{}, score: {}, e: {}\"\n","                  .format(e_test, TEST_Episodes, tot_rewards, 0))\n","            break;"]},{"cell_type":"markdown","source":["### Plotting"],"metadata":{"id":"vdIMSKntIbZB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVaSnNVlG1YG"},"outputs":[],"source":["#Plotting\n","rolling_average = np.convolve(rewards, np.ones(100)/100)\n","plt.figure(figsize=(15,8))\n","plt.plot(rewards)\n","plt.plot(rolling_average, label='Double Q-learning', color='black')\n","plt.axhline(y=195, color='r', linestyle='-') #Solved Line\n","#Scale Epsilon (0.001 - 1.0) to match reward (0 - 200) range\n","eps_graph = [200*x for x in epsilons]\n","plt.plot(eps_graph, label='epsilon', color='g', linestyle='-')\n","#Plot the line where TESTING begins\n","plt.axvline(x=TRAIN_END, color='y', linestyle='-')\n","plt.xlim( (0,EPISODES) )\n","plt.ylim( (0,220) )\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"VgSwVENIPcM6"},"source":["# @title  What is the primary motivation behind Double Q-learning? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"\" #@param [\"\",\"To speed up the convergence of the Q-learning algorithm.\",\" To address the overestimation bias in Q-values.\", \" To reduce the size of the Q-table.\", \"To improve exploration in the learning process.\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}