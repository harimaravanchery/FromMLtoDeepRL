{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1cRexxEiUVQ7kVNoQAlfIe4mtUx9xol5Z","timestamp":1725762961291},{"file_id":"1tE0An5wWouG3AecV61RrDSqRHMfBvL-r","timestamp":1725735306769}],"gpuType":"T4","authorship_tag":"ABX9TyO2zBbJVdfVENZS9eakKc0F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **A2C Algorithm (pytorch)**\n","\n","\n","This notebook demonstrates how to implement the Advantage Actor-Critic (A2C) algorithm for the CartPole-v1 from OpenAI Gym environment using PyTorch.\n","\n","1. **Visualize A2C Algorithm**: Visualization of the A2C algorithm's architecture using Graphviz.\n","2. **Imports and Helper Functions**: Essential libraries are imported, and helper functions are defined for tensor conversion, video rendering, and animation display.\n","3. **Network Definitions**: Neural network architectures for the Actor and Critic are defined.\n","4. **Memory Class**: A custom memory class to store experience samples.\n","5. **Environment and Initializatio**n: The environment is initialized, and Actor and Critic networks are set up.\n","6. **Training Function**: The train function updates the Actor and Critic networks based on stored experiences.\n","7. **Training Loop**: The main training loop where the agent interacts with the environment, collects experiences, and updates models. It also captures video frames of the training process.\n","8. **Save and Display Video**: Functions to save and display the recorded video of the training process in Google Colab.\n","9. **Plot Rewards**: Visualization of the total rewards per episode to analyze the training performance.\n","10. **Display Animation**: Display an animation of the training process using the collected video frames.\n","\n","\n","\n","\n"],"metadata":{"id":"QHn6YKDiJJst"}},{"cell_type":"markdown","source":["# Visualize A2C Algorithm"],"metadata":{"id":"bWyA1hFG84IY"}},{"cell_type":"code","source":["# Visualize A2C Algorithm\n","import graphviz\n","graphviz.__version__, graphviz.version()\n","from graphviz import Digraph\n","\n","# Define the graph\n","dot = Digraph(name='A2C_Algorithm', format='png')\n","\n","# Add nodes\n","dot.node('environment', label='Environment')\n","dot.node('actor', label='Actor')\n","dot.node('critic', label='Critic')\n","dot.node('policy_update', label='Policy Update')\n","dot.node('critic_update', label='Critic Update')\n","\n","# Add edges with labels\n","dot.edge('environment', 'actor', label='State (s_t)')\n","dot.edge('actor', 'environment', label='Action (a_t)')\n","dot.edge('environment', 'critic', label='(s_t, a_t)')\n","dot.edge('critic', 'policy_update', label='Critic Value')\n","dot.edge('actor', 'policy_update', label='Log Probability')\n","dot.edge('policy_update', 'actor', label='Actor Update')\n","dot.edge('critic', 'critic_update', label='TD Error')\n","dot.edge('critic_update', 'critic', label='Critic Update', style='dashed')\n","\n","# Render the graph\n","dot.render('a2c_diagram.png', view=True)\n","\n","print(\"A2C Algorithm block diagram created successfully!\")\n","\n","dot"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":330},"id":"dNBWh3lq8zCH","executionInfo":{"status":"ok","timestamp":1725720478228,"user_tz":-330,"elapsed":378,"user":{"displayName":"hari","userId":"15787394902795857400"}},"outputId":"09d8e4dd-7a21-42bb-b205-0e8589a44e76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A2C Algorithm block diagram created successfully!\n"]},{"output_type":"execute_result","data":{"image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: A2C_Algorithm Pages: 1 -->\n<svg width=\"434pt\" height=\"218pt\"\n viewBox=\"0.00 0.00 433.70 218.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 214)\">\n<title>A2C_Algorithm</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-214 429.7,-214 429.7,4 -4,4\"/>\n<!-- environment -->\n<g id=\"node1\" class=\"node\">\n<title>environment</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"191.7\" cy=\"-192\" rx=\"57.39\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"191.7\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">Environment</text>\n</g>\n<!-- actor -->\n<g id=\"node2\" class=\"node\">\n<title>actor</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"112.7\" cy=\"-105\" rx=\"30.59\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"112.7\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">Actor</text>\n</g>\n<!-- environment&#45;&gt;actor -->\n<g id=\"edge1\" class=\"edge\">\n<title>environment&#45;&gt;actor</title>\n<path fill=\"none\" stroke=\"black\" d=\"M146.78,-180.67C133.96,-175.45 121.4,-167.64 113.7,-156 109.31,-149.35 107.93,-141.05 107.98,-133.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111.48,-133.33 108.73,-123.1 104.5,-132.81 111.48,-133.33\"/>\n<text text-anchor=\"middle\" x=\"141.7\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">State (s_t)</text>\n</g>\n<!-- critic -->\n<g id=\"node3\" class=\"node\">\n<title>critic</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"293.7\" cy=\"-105\" rx=\"30.59\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"293.7\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">Critic</text>\n</g>\n<!-- environment&#45;&gt;critic -->\n<g id=\"edge3\" class=\"edge\">\n<title>environment&#45;&gt;critic</title>\n<path fill=\"none\" stroke=\"black\" d=\"M220.56,-176.38C230.58,-170.67 241.58,-163.65 250.7,-156 259.74,-148.42 268.47,-138.84 275.68,-130.09\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"278.58,-132.06 282.08,-122.06 273.11,-127.7 278.58,-132.06\"/>\n<text text-anchor=\"middle\" x=\"289.2\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">(s_t, a_t)</text>\n</g>\n<!-- actor&#45;&gt;environment -->\n<g id=\"edge2\" class=\"edge\">\n<title>actor&#45;&gt;environment</title>\n<path fill=\"none\" stroke=\"black\" d=\"M136.4,-116.4C147.66,-122.29 160.62,-130.59 169.7,-141 175.55,-147.71 180.07,-156.22 183.45,-164.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"180.22,-165.68 187.01,-173.81 186.78,-163.22 180.22,-165.68\"/>\n<text text-anchor=\"middle\" x=\"213.2\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">Action (a_t)</text>\n</g>\n<!-- policy_update -->\n<g id=\"node4\" class=\"node\">\n<title>policy_update</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"112.7\" cy=\"-18\" rx=\"61.19\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"112.7\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Policy Update</text>\n</g>\n<!-- actor&#45;&gt;policy_update -->\n<g id=\"edge5\" class=\"edge\">\n<title>actor&#45;&gt;policy_update</title>\n<path fill=\"none\" stroke=\"black\" d=\"M82.19,-103.34C56.86,-100.97 22.26,-93.15 4.7,-69 -10.87,-47.59 16.21,-35.08 47.13,-27.94\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"48.2,-31.29 57.26,-25.8 46.76,-24.44 48.2,-31.29\"/>\n<text text-anchor=\"middle\" x=\"47.7\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">Log Probability</text>\n</g>\n<!-- critic&#45;&gt;policy_update -->\n<g id=\"edge4\" class=\"edge\">\n<title>critic&#45;&gt;policy_update</title>\n<path fill=\"none\" stroke=\"black\" d=\"M270.56,-93.05C249.09,-82.9 216.26,-67.4 187.7,-54 176.52,-48.76 164.4,-43.09 153.22,-37.87\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"154.64,-34.67 144.1,-33.62 151.68,-41.02 154.64,-34.67\"/>\n<text text-anchor=\"middle\" x=\"249.2\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">Critic Value</text>\n</g>\n<!-- critic_update -->\n<g id=\"node5\" class=\"node\">\n<title>critic_update</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"334.7\" cy=\"-18\" rx=\"58.49\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"334.7\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Critic Update</text>\n</g>\n<!-- critic&#45;&gt;critic_update -->\n<g id=\"edge7\" class=\"edge\">\n<title>critic&#45;&gt;critic_update</title>\n<path fill=\"none\" stroke=\"black\" d=\"M292.56,-87.01C292.56,-76.95 293.78,-64.21 298.7,-54 300.72,-49.82 303.45,-45.87 306.51,-42.23\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"309.24,-44.45 313.6,-34.8 304.17,-39.62 309.24,-44.45\"/>\n<text text-anchor=\"middle\" x=\"324.2\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">TD Error</text>\n</g>\n<!-- policy_update&#45;&gt;actor -->\n<g id=\"edge6\" class=\"edge\">\n<title>policy_update&#45;&gt;actor</title>\n<path fill=\"none\" stroke=\"black\" d=\"M112.7,-36.18C112.7,-47.81 112.7,-63.42 112.7,-76.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"109.2,-76.8 112.7,-86.8 116.2,-76.8 109.2,-76.8\"/>\n<text text-anchor=\"middle\" x=\"149.7\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">Actor Update</text>\n</g>\n<!-- critic_update&#45;&gt;critic -->\n<g id=\"edge8\" class=\"edge\">\n<title>critic_update&#45;&gt;critic</title>\n<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M346.18,-35.93C351.46,-45.96 355.42,-58.7 349.7,-69 344.84,-77.78 336.75,-84.65 328.21,-89.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"326.36,-86.91 319.26,-94.77 329.72,-93.05 326.36,-86.91\"/>\n<text text-anchor=\"middle\" x=\"388.7\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">Critic Update</text>\n</g>\n</g>\n</svg>\n","text/plain":["<graphviz.graphs.Digraph at 0x78fe88546a10>"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["The **A2C (Advantage Actor-Critic)** algorithm diagram visually represents the key components and data flow involved in the A2C reinforcement learning algorithm. Here’s a summary of the diagram's components and their interactions:\n","\n","**Environment**: The environment interacts with the agent by providing states and rewards. It receives actions from the Actor and sends state-action pairs to the Critic.\n","\n","**Actor**: The Actor network decides which action to take based on the current state. It receives the state from the environment and sends the chosen action back. The Actor also receives updates from the Policy Update step to improve its policy.\n","\n","**Critic**: The Critic network estimates the value of states or state-action pairs. It receives state-action pairs from the environment and provides value estimates to the Policy Update step. The Critic is updated based on the TD (temporal difference) error.\n","\n","**Policy Update**: This step updates the Actor based on the computed advantage function, which measures how good the taken actions are relative to the estimated values. The Policy Update step adjusts the Actor’s policy to maximize rewards.\n","\n","**Critic Update**: This step refines the Critic’s value function based on the TD error, which is the difference between the predicted value and the actual reward plus the discounted value of the next state.\n","\n","**Data Flow**:\n","\n","The Environment sends the state to the Actor and receives actions from it.\n","\n","The Actor provides actions to the Environment and logs probabilities of those actions.\n","\n","The Environment provides state-action pairs to the Critic for value estimation.\n","\n","The Critic calculates the TD error and provides value estimates to the Policy Update step.\n","\n","The Policy Update step uses the advantage function and the Actor’s log probabilities to update the Actor.\n","\n","The Critic is updated based on the TD error received from the Critic Update step.\n","\n","The diagram encapsulates the iterative process of updating both the Actor and Critic networks to optimize the policy and value function for improved decision-making in reinforcement learning."],"metadata":{"id":"IkduD7ZvRCer"}},{"cell_type":"markdown","source":["# Imports and Helper Function"],"metadata":{"id":"2zbmwMhfIAMt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJtZQ9mLHx7w"},"outputs":[],"source":["# General Summary\n","# This notebook demonstrates training a reinforcement learning agent using a policy gradient method on the CartPole-v1 environment from OpenAI's Gym.\n","# The notebook includes setup for neural network models, a memory class to store experience, training routines, and video recording of agent performance.\n","\n","# Cell 1: Imports and Helper Functions\n","import numpy as np\n","import torch\n","import gym\n","from torch import nn\n","import matplotlib.pyplot as plt\n","from IPython.display import display, HTML\n","import cv2\n","import matplotlib.animation as animation\n","\n","# Helper function to convert numpy arrays to tensors\n","def t(x): return torch.tensor(x, dtype=torch.float32)\n","\n","# Function to display animation in Google Colab\n","def display_animation(frames, interval=50):\n","    fig, ax = plt.subplots()\n","    img = ax.imshow(frames[0])\n","\n","    def update(frame):\n","        img.set_data(frame)\n","        return [img]\n","\n","    ani = animation.FuncAnimation(fig, update, frames=frames, interval=interval)\n","    return HTML(ani.to_jshtml())"]},{"cell_type":"markdown","source":["# Network Definitions"],"metadata":{"id":"-NwZIcsRIHBb"}},{"cell_type":"code","source":["# Network Definitions\n","# This cell defines the Actor and Critic neural network modules for the reinforcement learning agent.\n","\n","class Actor(nn.Module):\n","    def __init__(self, state_dim, action_dim):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(state_dim, 64),\n","            nn.Tanh(),\n","            nn.Linear(64, 64),\n","            nn.Tanh(),\n","            nn.Linear(64, action_dim)\n","        )\n","        self.log_std = nn.Parameter(torch.zeros(action_dim))\n","\n","    def forward(self, X):\n","        mean = self.model(X)\n","        std = torch.exp(self.log_std)\n","        return mean, std\n","\n","class Critic(nn.Module):\n","    def __init__(self, state_dim):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(state_dim, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 1)\n","        )\n","\n","    def forward(self, X):\n","        return self.model(X)\n"],"metadata":{"id":"sXHh1GRsILPb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Memory Class"],"metadata":{"id":"4fgBruoxIN3r"}},{"cell_type":"code","source":["# Memory Class\n","# This cell defines the Memory class used to store experience tuples (state, action, reward, etc.) during training.\n","\n","class Memory:\n","    def __init__(self):\n","        self.states = []\n","        self.actions = []\n","        self.log_probs = []\n","        self.values = []\n","        self.rewards = []\n","        self.dones = []\n","\n","    def add(self, state, action, log_prob, value, reward, done):\n","        self.states.append(state)\n","        self.actions.append(action)\n","        self.log_probs.append(log_prob)\n","        self.values.append(value)\n","        self.rewards.append(reward)\n","        self.dones.append(done)\n","\n","    def clear(self):\n","        del self.states[:]\n","        del self.actions[:]\n","        del self.log_probs[:]\n","        del self.values[:]\n","        del self.rewards[:]\n","        del self.dones[:]\n","\n","    def __len__(self):\n","        return len(self.rewards)\n","\n","    def reversed(self):\n","        return reversed(list(zip(self.states, self.actions, self.log_probs, self.values, self.rewards, self.dones)))"],"metadata":{"id":"npMvW_fkIP88"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Environment and Initialization"],"metadata":{"id":"zacJJ-hlIT2c"}},{"cell_type":"code","source":["# Environment and Initialization\n","# This cell initializes the CartPole-v1 environment and sets up the Actor and Critic networks along with their optimizers.\n","\n","env = gym.make(\"CartPole-v1\")\n","\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.n  # Discrete action space\n","\n","# Initialize actor and critic\n","actor = Actor(state_dim, action_dim)\n","critic = Critic(state_dim)\n","adam_actor = torch.optim.Adam(actor.parameters(), lr=1e-4)\n","adam_critic = torch.optim.Adam(critic.parameters(), lr=1e-3)\n","\n","gamma = 0.99\n","memory = Memory()\n","max_steps = 200\n","batch_size = 64\n","episode_rewards = []"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U0T2i7jWIiNU","executionInfo":{"status":"ok","timestamp":1725709791633,"user_tz":-330,"elapsed":439,"user":{"displayName":"hari","userId":"15787394902795857400"}},"outputId":"4336b2c7-2c2f-4d7b-e036-af4ac1ff427a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"markdown","source":["# Training Function"],"metadata":{"id":"wvjkmKGFIjrz"}},{"cell_type":"code","source":["# Training Function\n","# This cell contains the function to train the Actor and Critic networks using the experiences stored in the memory.\n","\n","def train(memory, q_vals):\n","    values = torch.stack(memory.values)\n","    q_vals = torch.tensor(q_vals, dtype=torch.float32)\n","    advantage = q_vals - values\n","\n","    # Critic loss\n","    critic_loss = advantage.pow(2).mean()\n","    adam_critic.zero_grad()\n","    critic_loss.backward()\n","    adam_critic.step()\n","\n","    # Actor loss\n","    actor_loss = (-torch.stack(memory.log_probs) * advantage.detach()).mean()\n","    adam_actor.zero_grad()\n","    actor_loss.backward()\n","    adam_actor.step()"],"metadata":{"id":"VEB7BsG1Im6S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Loop"],"metadata":{"id":"0Km6P9ZnIqUt"}},{"cell_type":"code","source":["# Training Loop\n","# This cell runs the training loop for the reinforcement learning agent. It collects experience, trains the networks, and records video frames.\n","\n","def run_training(episodes=1000):\n","    for episode in range(episodes):\n","        state = env.reset()\n","        done = False\n","        total_reward = 0\n","        steps = 0\n","        video_frames = []\n","\n","        while not done:\n","            state_tensor = t(state)\n","            mean, _ = actor(state_tensor)\n","            dist = torch.distributions.Categorical(logits=mean)\n","            action = dist.sample()\n","\n","            next_state, reward, done, _ = env.step(action.item())\n","            total_reward += reward\n","            steps += 1\n","            memory.add(state_tensor, action, dist.log_prob(action), critic(state_tensor), reward, done)\n","\n","            video_frames.append(env.render(mode='rgb_array'))\n","\n","            if done or (steps % max_steps == 0):\n","                if len(memory) >= batch_size:\n","                    last_value = critic(t(next_state)).detach()\n","                    q_vals = np.zeros(len(memory))\n","\n","                    for i, (_, _, _, _, reward, done) in enumerate(memory.reversed()):\n","                        last_value = reward + gamma * last_value * (1.0 - done)\n","                        q_vals[len(memory) - 1 - i] = last_value\n","\n","                    train(memory, q_vals)\n","                    memory.clear()\n","\n","        episode_rewards.append(total_reward)\n","\n","        if episode == episodes - 1:\n","            return video_frames\n","\n","# Run the training\n","video_frames = run_training()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJp3qhQpIss8","executionInfo":{"status":"ok","timestamp":1725714000642,"user_tz":-330,"elapsed":4201446,"user":{"displayName":"hari","userId":"15787394902795857400"}},"outputId":"68cd7e16-aee0-4457-8b72-8976c10e7de4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"markdown","source":["# Recording and Displaying the Video"],"metadata":{"id":"cEiMhmYrIuhc"}},{"cell_type":"code","source":["# Save and Display Video\n","# This cell saves the recorded video of the agent's performance and displays it with a replay button.\n","\n","def save_video(frames, filename, fps=30):\n","    height, width, _ = frames[0].shape\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    video = cv2.VideoWriter(filename, fourcc, fps, (width, height))\n","\n","    for frame in frames:\n","        video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n","\n","    video.release()\n","\n","def display_video_with_replay(filename):\n","    video_html = f\"\"\"\n","    <video id=\"videoPlayer\" width=\"600\" controls>\n","        <source src=\"{filename}\" type=\"video/mp4\">\n","    </video>\n","    <br>\n","    <button onclick=\"document.getElementById('videoPlayer').currentTime=0;\">Replay Video</button>\n","    \"\"\"\n","    display(HTML(video_html))\n","\n","# Save the video\n","save_video(video_frames, '/content/sac_cartpole.mp4')\n","\n","# Display the video with a replay button\n","display_video_with_replay('/content/sac_cartpole.mp4')\n"],"metadata":{"colab":{"resources":{"http://localhost:8080/content/a2c_mountaincar_continuous.mp4":{"data":"","ok":false,"headers":[["content-length","0"]],"status":404,"status_text":""}},"base_uri":"https://localhost:8080/","height":342},"id":"33DQNH6aIw_6","executionInfo":{"status":"ok","timestamp":1725715177330,"user_tz":-330,"elapsed":1677,"user":{"displayName":"hari","userId":"15787394902795857400"}},"outputId":"d84972c0-e484-4941-ce77-b0ddfb9dbefc"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <video id=\"videoPlayer\" width=\"600\" controls>\n","        <source src=\"/content/a2c_mountaincar_continuous.mp4\" type=\"video/mp4\">\n","    </video>\n","    <br>\n","    <button onclick=\"document.getElementById('videoPlayer').currentTime=0;\">Replay Video</button>\n","    "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_b3226e40-313e-4e78-8283-89f24723834f\", \"a2c_mountaincar_continuous.mp4\", 1529156)"]},"metadata":{}}]},{"cell_type":"markdown","source":["# Plot Rewards"],"metadata":{"id":"UENS4Z7_SWef"}},{"cell_type":"code","source":["# Plot Rewards\n","# This cell plots the total rewards per episode to visualize the training progress.\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(episode_rewards, label='Total Reward')\n","plt.title(\"Total reward per episode\")\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Total Reward\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"pbItgOODSZbR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Display Animation\n"],"metadata":{"id":"4JJfxvEoSeKz"}},{"cell_type":"code","source":["display(display_animation(video_frames))"],"metadata":{"id":"7KqiWUNfTWCs"},"execution_count":null,"outputs":[]}]}