{"cells":[{"cell_type":"markdown","metadata":{"id":"Cbh-3MVhGjvM"},"source":["# Fire Size Class Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RLdFbi4GjvQ"},"outputs":[],"source":["# If a library is missing on your system, you can install it with the command :\n","# ! pip install <library name>\n","# For instance (remove # on the following line before executing)\n","# ! pip install pandas"]},{"cell_type":"markdown","metadata":{"id":"jZWDlWidGjvS"},"source":["If you face an error when running the previous command line, try :\n","- `! pip install --proxy=http://cias3basic.conti.de:8080/wpad.dat <library name>`\n","- you may also want to run the install command by deactivating the VPN connection, multiple people have reported facing difficulties downloading packages due to firewalls."]},{"cell_type":"markdown","metadata":{"id":"rCJKSzuGGjvS"},"source":["## 1. Data preparation\n","\n","### 1.1 Data loading\n","\n","The data we use is usually split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model’s prediction on this subset.\n","\n","For this challenge, the division of the database into training and test datasets is not your responsibility. We provide you with the 2 separate CSV files.\n","CSV files can be easily read with the Pandas library. The resulting object is called a DataFrame.\n","\n","For more info about the DataFrame object and functions, you can go here : https://www.tutorialspoint.com/python_pandas/python_pandas_dataframe.htm"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"3B1-NImrGjvT","executionInfo":{"status":"error","timestamp":1703216976796,"user_tz":-330,"elapsed":1223,"user":{"displayName":"hari","userId":"15787394902795857400"}},"outputId":"cb24b3c6-08d0-4877-de76-cf0bc98e3bdd"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-118ab227b1ff>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"]}],"source":["import pandas as pd\n","\n","train = pd.read_csv('train.csv')\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P4bwv3p2GjvT"},"outputs":[],"source":["test = pd.read_csv('test.csv')\n","test.head()"]},{"cell_type":"markdown","metadata":{"id":"m8w8lWl8GjvT"},"source":["As you can see, the test dataset does not contain the size_class, it will be your objective to predict it.\n","\n","We can print the dataset sizes as well :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXWjDudcGjvT"},"outputs":[],"source":["print('Train set size:', train.shape)\n","print('Test set size:', test.shape)"]},{"cell_type":"markdown","metadata":{"id":"VZ9OcecaGjvU"},"source":["### 1.2 Feature selection\n","\n","Pay attention that our features are of different types - some of them are numeric, some are categorical, and some are even just strings, which normally should be handled in some specific way.\n","\n","Indeed, categorical data are commonplace in many Data Science and Machine Learning problems but are usually more challenging to deal with than numerical data. In particular, many machine learning algorithms require that their input is numerical and therefore categorical features must be transformed into numerical features before we can use any of these algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0vnwO0tuGjvU"},"outputs":[],"source":["print(train.dtypes)"]},{"cell_type":"markdown","metadata":{"id":"CFthJVh0GjvU"},"source":["For getting started with a simple model, we will only use 2 numerical features (fire_location_latitude, fire_location_longitude) and add a few categorical features (fire_origin, true_cause, fire_type, fuel_type, weather_conditions_over_fire).\n","\n","Now let's separate features and label variable. In most ML scenarios, X refers to the array of features and y is the variable to be predicted."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6L-qyybKGjvU"},"outputs":[],"source":["X = train[['fire_location_latitude', 'fire_location_longitude', 'fire_origin', 'true_cause', 'fire_type', 'weather_conditions_over_fire', 'fuel_type']]\n","X.head()"]},{"cell_type":"markdown","metadata":{"id":"phBYVQNAGjvV"},"source":["In this challenge, the goal is to predict the __size class__ of the burned area, which consists in various classes :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNkZcOTPGjvV"},"outputs":[],"source":["y = train['size_class']\n","y.unique()"]},{"cell_type":"markdown","metadata":{"id":"NmQo-k81GjvV"},"source":["This is a __supervised classification problem__ with 5 classes to predict :\n","- A class = 0 to 0.1 ha (1 hectare = 10000 sq meters = 2.5 acres)\n","- B class > 0.1 ha to 4.0 ha\n","- C class > 4.0 ha to 40.0 ha\n","- D class > 40.0 ha to 200 ha\n","- E class > 200 ha\n","\n","To learn more about the different types of problems in Machine Learning, you can go here :\n","https://www.softwaretestinghelp.com/types-of-machine-learning-supervised-unsupervised/"]},{"cell_type":"markdown","metadata":{"id":"Hv2uvwufGjvV"},"source":["### 1.3 Dealing with missing values\n","\n","Let's check how many absent values do we have:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lf1FZK9zGjvV"},"outputs":[],"source":["null_value_stats = X.isnull().sum(axis=0)\n","null_value_stats[null_value_stats != 0]"]},{"cell_type":"markdown","metadata":{"id":"NjaHrrVgGjvW"},"source":["As you can see, some features indeed have missing values.\n","\n","Missing Value treatment becomes important since the data insights or the performance of your predictive model could be impacted if the missing values are not appropriately handled.\n","\n","For now, let's just fill them with some arbitrary value so that the model would be able to easily distinguish between them and take it into account :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhdLQVkbGjvW"},"outputs":[],"source":["X = X.fillna('Unknown') # str"]},{"cell_type":"markdown","metadata":{"id":"pfGVgsuUGjvW"},"source":["### 1.4 Data Splitting\n","\n","A validation dataset is a sample of data held back from training your model that is used to give an estimate of model performance while tuning model’s hyperparameters.\n","\n","There is much confusion in applied ML about what a validation dataset is exactly and how it differs from a test dataset. Long story short :\n","- The validation dataset is predominately used to describe the evaluation of models when tuning hyperparameters and data preparation\n","- The test dataset is predominately used to describe the evaluation of a final tuned model when comparing it to other final models.\n","\n","A good justification of this method can be found here : https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n","\n","So let's split the train data into training and validation sets :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-qJZKy_GjvW"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.75, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"ESJZ9TQgGjvX"},"source":["### 1.5 Categorical feature encoding\n","\n","As mentioned before, many machine learning algorithms require that their input is numerical and therefore categorical features must be transformed into numerical features before we can use any of these algorithms.\n","\n","One of the most common ways to make this transformation is to __one-hot encode__ the categorical features, especially when there does not exist a natural ordering between the categories (e.g. a feature ‘City’ with names of cities such as ‘London’, ‘Lisbon’, ‘Berlin’, etc.). For each unique value of a feature (say, ‘London’) one column is created (say, ‘City_London’) where the value is 1 if for that instance the original feature takes that value and 0 otherwise.\n","\n","A simple way to make this transformation is to use the category encoders library : http://contrib.scikit-learn.org/category_encoders/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8_oWrGc1GjvX"},"outputs":[],"source":["import category_encoders as ce\n","\n","ohe = ce.OneHotEncoder(handle_unknown='value', use_cat_names=True)\n","X_train_ohe = ohe.fit_transform(X_train)\n","X_train_ohe.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"hzWaVnsWGjvX"},"source":["When we try to transform the test set, after having fitted the encoder to the training set, we may obtain a ValueError. This is because there are new, previously unseen unique values in the test set and the encoder doesn’t know how to handle these values. In order to use both the transformed training and test sets in machine learning algorithms, we need them to have the __same number of columns__.\n","\n","This last problem can be solved by using the option `handle_unknown='value'` of the OneHotEncoder, which will ignore previously unseen values when transforming the test set and fill the resulting one-hot encoded columns for this feature with zeros."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22qYiUM-GjvX"},"outputs":[],"source":["X_valid_ohe = ohe.transform(X_valid)\n","X_valid_ohe.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"isT5zBj8GjvX"},"source":["As remarked previously, one-hot encoding is not the only possible way to encode categorical features and the category encoders library has several encoders which you should explore, as others might be more appropriate for different categorical features and machine learning problems."]},{"cell_type":"markdown","metadata":{"id":"WUWvWTegGjvY"},"source":["## 2. Model Training\n","\n","For this example, we will use the DecisionTreeClassifier from scikit-learn, which belongs to the family of supervised learning algorithms.\n","\n","The goal of using a Decision Tree is to create a training model able to predict the class or value of the target variable by learning simple decision rules inferred from prior data (training data). To learn more about this algorithm :\n","https://www.mygreatlearning.com/blog/decision-tree-algorithm/\n","\n","Note that we fixed the random_state value only to make the results reproducible for this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"id7gq_BKGjvY"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","model = DecisionTreeClassifier(random_state=42)\n","model.fit(X_train_ohe, y_train)"]},{"cell_type":"markdown","metadata":{"id":"WYdkIPJSGjvY"},"source":["You can get a look at the feature importances and plot the decision tree as well.\n","\n","### 2.1 Feature importance\n","\n","Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdNHKGwJGjvY"},"outputs":[],"source":["importance = pd.DataFrame()\n","importance['Feature'] = X_train_ohe.columns\n","importance['Importance'] = model.feature_importances_\n","importance.set_index('Feature', inplace=True)\n","\n","importance.sort_values(by='Importance', ascending=False)"]},{"cell_type":"markdown","metadata":{"id":"JO25s7q7GjvY"},"source":["The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance."]},{"cell_type":"markdown","metadata":{"id":"OFqfu5tSGjvZ"},"source":["### 2.2 Plotting Tree\n","\n","For clarity and in order to save time, we will only generate the first 3 levels of the tree. If `max_depth=None`, the tree is fully generated.\n","\n","For this baseline model, it takes about 8 min to plot the full decision tree, but the result is horrible and practically unreadable !"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_lOCmQOGjvZ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.tree import plot_tree\n","\n","plt.figure(figsize=(20,15))\n","plot_tree(model, max_depth=3, feature_names=X_train_ohe.columns, filled=True, fontsize=10)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"n43ogxz4GjvZ"},"source":["## 3. Model Validation\n","\n","### 3.1 Predict on Validation dataset\n","All you have to do to get predictions on your validation set is :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4v-1Bz51Gjva"},"outputs":[],"source":["y_valid_pred = model.predict(X_valid_ohe)"]},{"cell_type":"markdown","metadata":{"id":"v1uyoB3yGjva"},"source":["### 3.2 Evaluation"]},{"cell_type":"markdown","metadata":{"id":"93s1B9u4Gjva"},"source":["Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGP5npo8Gjva"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","print('Validation accuracy score:', accuracy_score(y_valid, y_valid_pred))"]},{"cell_type":"markdown","metadata":{"id":"-yCHIzcYGjvb"},"source":["A problem with this metric is that it doesn't take in consideration the fact that some classes are more represented than others, and unfortunately, it can actually lead to biased results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qbiKoyB_Gjvm"},"outputs":[],"source":["pd.DataFrame(y_valid, columns=['size_class'])['size_class'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"LfAvZ8xSGjvm"},"source":["Indeed, most machine learning algorithms assume data is equally distributed. So when we have a class imbalance, the machine learning classifier tends to be more biased towards the majority class, causing bad classification of the minority class.\n","\n","To get a deeper understanding of this problem, you can go here : https://towardsdatascience.com/class-imbalance-a-classification-headache-1939297ff4a4\n","\n","So in order to weight all classes equally, the evaluation metric for this challenge will be the __macro-average F1-score__ from scikit-learn :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UjEBhyTSGjvm"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","print('Macro F1-score:', f1_score(y_valid, y_valid_pred, average='macro'))"]},{"cell_type":"markdown","metadata":{"id":"wfQYJMnGGjvn"},"source":["Note that the F1-score with uniform weights is lower (0.29) than the overall accuracy (0.63) because it gives equal contribution to the predictive performance for the five classes, independent of their number of observations.\n","\n","__Remember that this metric should be maximized.__\n","\n","A confusion matrix is also a predictive analytics tool. Specifically, it is a table that displays and compares actual values with the model’s predicted values. Within the context of machine learning, a confusion matrix is utilized as a metric to analyze how a machine learning classifier performed on a dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q05vq1O4Gjvn"},"outputs":[],"source":["from sklearn.metrics import plot_confusion_matrix\n","\n","class_names = y_valid.unique()\n","\n","# Plot non-normalized and normalized confusion matrices\n","titles_options = [(\"Confusion matrix, without normalization\", None),\n","                  (\"Normalized confusion matrix\", 'true')]\n","\n","for title, normalize in titles_options:\n","    disp = plot_confusion_matrix(model, X_valid_ohe, y_valid,\n","                                 labels=class_names,\n","                                 cmap=plt.cm.Blues,\n","                                 normalize=normalize)\n","    disp.ax_.set_title(title)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BjYJGL3SGjvn"},"source":["The classes that the model predicted correctly can be read on the diagonal on the matrix."]},{"cell_type":"markdown","metadata":{"id":"-mZud8GOGjvn"},"source":["## 4. Make submission\n","\n","### 4.1 Predict on Test dataset\n","Now we would re-train our model on all train data that we have."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e_Yf0lwSGjvn"},"outputs":[],"source":["# Categorical Feature Encoding\n","X_ohe = ohe.fit_transform(X)\n","\n","# Model Training\n","model.fit(X_ohe, y)"]},{"cell_type":"markdown","metadata":{"id":"Lj3gAIVfGjvn"},"source":["Don't forget to apply the same data transformations you used on your training set before applying the model on your test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5UUD759yGjvo"},"outputs":[],"source":["# Feature Selection\n","X_test = test[['fire_location_latitude', 'fire_location_longitude', 'fire_origin', 'true_cause', 'fire_type', 'weather_conditions_over_fire', 'fuel_type']]\n","\n","# Missing values treatment\n","X_test = X_test.fillna('Unknown') # str\n","X_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcTY6xArGjvo"},"outputs":[],"source":["# Categorical Feature Encoding\n","X_test_ohe = ohe.transform(X_test)\n","X_test_ohe.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"c1mgjePFGjvo"},"source":["And finally let's prepare the submission file:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLfIlGgXGjvo"},"outputs":[],"source":["y_test_pred = pd.DataFrame()\n","y_test_pred['fire_number'] = test['fire_number']\n","y_test_pred['size_class'] = model.predict(X_test_ohe)\n","\n","y_test_pred.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wrue7IfnGjvo"},"outputs":[],"source":["print(y_test_pred.shape)"]},{"cell_type":"markdown","metadata":{"id":"VjYhjS2uGjvp"},"source":["You should have 3969 rows in your submission file."]},{"cell_type":"markdown","metadata":{"id":"VZJ9UVVxGjvp"},"source":["### 4.2 Push the prediction to CARINO platform\n","\n","First you need to set the TOKEN value to the value you will find on the platform under the _Submissions_ tab :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDmYE6F5Gjvp"},"outputs":[],"source":["import math\n","import requests\n","\n","TOKEN = None # Replace with 'your token' (please ensure value is between quotes)\n","comment = 'some comment' # Free text, visible by you only\n","\n","def submit_prediction(df, TOKEN=None, sep=',', comment='', compression='gzip', **kwargs):\n","    if TOKEN is None:\n","        print(\"Please set TOKEN to the value provided in CARINO plarform under Submissions tab\")\n","        return None\n","    URL='http://18.185.86.47/api/submissions'\n","    df.to_csv('temporary.dat', sep=sep, compression=compression, **kwargs)\n","    r = requests.post(URL, headers={'Authorization': 'Bearer {}'.format(TOKEN)},files={'datafile': open('temporary.dat', 'rb')},data={'comment':comment, 'compression': compression})\n","    if r.status_code == 429:\n","        raise Exception('Submissions are too close. Next submission is only allowed in {} minutes.'.format(int(math.ceil(int(r.headers['x-rate-limit-remaining']) / 1000.0 / 60.0))))\n","    if r.status_code != 200:\n","        raise Exception(r.text)"]},{"cell_type":"markdown","metadata":{"id":"Lj1QCw-vGjvp"},"source":["You can then submit your results as follows :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wd-wIEhgGjvq"},"outputs":[],"source":["submit_prediction(y_test_pred, TOKEN, sep=',', index=True, comment=comment)"]},{"cell_type":"markdown","metadata":{"id":"075cdDxEGjvq"},"source":["You should now be able to check your score on the CARINO platform.\n","\n","Note that you can save your predictions as a CSV file and upload it manually on the platform under the _Submissions_ tab later as well :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcgNZZeFGjvq"},"outputs":[],"source":["y_test_pred.to_csv('submission.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"viswzpEhGjvq"},"source":["That's it ! Now you can play around with other models, add some feature engineering and try to improve your score ! :)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}